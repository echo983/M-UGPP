Metadata-Version: 2.4
Name: m-ugpp
Version: 0.1.0
Summary: Minimal UGPP engine with runnable demo roles and CLI
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: flask>=3.0.0
Requires-Dist: openai>=1.43.0
Provides-Extra: dev

# Minimal UGPP Demo

This repository contains a runnable, minimal implementation of the UGPP (Understanding, Guidance, Planning, Production) engine with simple role implementations and a CLI entrypoint.

## Quick start

1. Create a virtual environment and install the package in editable mode:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .
```

2. Run the pipeline with the `ugpp` command:

```bash
ugpp "Ship a demo feature"
```

> To use GPT-powered roles (default), set `OPENAI_API_KEY` in your shell before running:
> ```bash
> export OPENAI_API_KEY=sk-... # your key
> ```

Pass `--dump-json` to capture machine-readable output, or add repeated `--truth` and `--need` flags to seed the MTS for automated tests and debugging.

Example with seeded needs and truths:

```bash
ugpp "Improve docs" \
  --need need-docs|Clarify contribution steps|0.8|medium|docs-gap \
  --truth t1|Docs outline drafted|0.75
```

### Run bash or python maintenance tasks

The pipeline can now execute real tasks in order, making it usable for simple local ops:

- Run bash commands (in order):  
  `ugpp "Inspect workspace" --bash "pwd" --bash "ls -la" --workdir .`
- Run python helpers:  
  `ugpp "Check python version" --python "import sys; print(sys.version)" --task-timeout 10`
- Mix using `--task executor:payload` to control order explicitly:  
  `ugpp "Cleanup cache" --task "bash:du -sh ~/.cache" --task "python:import shutil; print('ready')"`.

The `--workdir` flag sets the working directory for all tasks, and `--task-timeout` limits each command. All outputs are captured in the final JSON payload for auditing.

### GPT-driven planning and execution

- GPT is enabled by default for all roles (discover, plan, execute, evaluate). Override models as needed:  
  `ugpp "Collect inventory" --planner-model gpt-5.1 --worker-model gpt-5-mini`
- Use `--no-gpt` to fall back to local deterministic roles when offline.
- Default model choices: discoverer `gpt-5-nano`, planner `gpt-5.1`, worker `gpt-5-mini`, evaluator `gpt-5-mini`.

Environment variables `OPENAI_API_KEY`, `ADA_BRAIN_MODEL`, and `ADA_CODER_MODEL` are not required for the demo roles but can be exported for downstream integrations:

```bash
export OPENAI_API_KEY=sk-...your-key...
export ADA_BRAIN_MODEL=gpt-5.1
export ADA_CODER_MODEL=gpt-5-mini
```
